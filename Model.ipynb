{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec06a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from skimage.feature import hog, graycomatrix, graycoprops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2c339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load images from: Cell_Images\n",
      "---\n",
      "Success! Loaded 27584 images.\n",
      "Example: The first image 'IMG_4171_63.jpg' has dimensions: (75, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the folder containing your dataset\n",
    "# Adjust this path to where you saved the images\n",
    "dataset_folder = 'Cell_Images'\n",
    "\n",
    "# We will store the loaded images in a list\n",
    "cell_loaded_images = []\n",
    "# It's also useful to store their filenames\n",
    "cell_image_filenames = []\n",
    "\n",
    "print(f\"Starting to load images from: {dataset_folder}\")\n",
    "\n",
    "# Loop through every file in the folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    \n",
    "    # Check if the file is an image (e.g., .jpg, .png)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        \n",
    "        # Create the full path to the image\n",
    "        image_path = os.path.join(dataset_folder, filename)\n",
    "        \n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        if image is not None:\n",
    "            # If loading was successful, add it to our list\n",
    "            cell_loaded_images.append(image)\n",
    "            cell_image_filenames.append(filename)\n",
    "        else:\n",
    "            print(f\"Warning: Could not load {filename}\")\n",
    "\n",
    "print(f\"---\")\n",
    "print(f\"Success! Loaded {len(cell_loaded_images)} images.\")\n",
    "print(f\"Example: The first image '{cell_image_filenames[0]}' has dimensions: {cell_loaded_images[0].shape}\")\n",
    "\n",
    "# ...existing code...\n",
    "df = pd.read_csv(\"final_labels_manual.csv\")\n",
    "\n",
    "def normalize_id(s):\n",
    "    return os.path.splitext(os.path.basename(str(s)).strip())[0].lower()\n",
    "\n",
    "# build label map keyed by basename without extension (normalized)\n",
    "label_map = { normalize_id(cid): lab for cid, lab in zip(df[\"cell_id\"], df[\"label\"]) }\n",
    "\n",
    "y_labels = []\n",
    "missing = []\n",
    "for fname in cell_image_filenames:\n",
    "    key = normalize_id(fname)\n",
    "    if key in label_map:\n",
    "        y_labels.append(int(label_map[key]))\n",
    "    else:\n",
    "        missing.append(fname)\n",
    "\n",
    "if missing:\n",
    "    raise KeyError(f\"{len(missing)} filenames not found in label_map. Examples: {missing[:10]}\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd3bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(cell):\n",
    "    \"\"\"\n",
    "    Extracts a combined feature vector from a single 100x75 image cell.\n",
    "    \n",
    "    Features:\n",
    "    1. HSV Stats (6 features)\n",
    "    2. GLCM Properties (4 features)\n",
    "    3. HOG Features (192 features)\n",
    "    4. Edge Density (1 feature)\n",
    "    5. Harris Corner Count (1 feature)\n",
    "    6. FAST Keypoint Count (1 feature)\n",
    "    \n",
    "    Total Features: 6 + 4 + 192 + 1 + 1 + 1 = 205 features\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 0. Prerequisites ---\n",
    "    # Create a grayscale version for features that need it\n",
    "    gray_cell = cv2.cvtColor(cell, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Ensure grayscale is 8-bit unsigned int (required for GLCM)\n",
    "    gray_cell_uint8 = cv2.convertScaleAbs(gray_cell)\n",
    "    \n",
    "    feature_vector = []\n",
    "\n",
    "    try:\n",
    "        # --- 1. HSV Stats (6 features) ---\n",
    "        hsv_cell = cv2.cvtColor(cell, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv_cell)\n",
    "        hsv_stats = [\n",
    "            np.mean(h), np.std(h),\n",
    "            np.mean(s), np.std(s),\n",
    "            np.mean(v), np.std(v)\n",
    "        ]\n",
    "        feature_vector.extend(hsv_stats)\n",
    "        \n",
    "        # --- 2. GLCM Properties (4 features) ---\n",
    "        # \n",
    "        # Calculate GLCM\n",
    "        glcm = graycomatrix(gray_cell_uint8, \n",
    "                            distances=[1], \n",
    "                            angles=[0], \n",
    "                            levels=256, \n",
    "                            symmetric=True, \n",
    "                            normed=True)\n",
    "        \n",
    "        # Calculate properties\n",
    "        glcm_feats = [\n",
    "            graycoprops(glcm, 'contrast')[0, 0],\n",
    "            graycoprops(glcm, 'homogeneity')[0, 0],\n",
    "            graycoprops(glcm, 'energy')[0, 0],\n",
    "            graycoprops(glcm, 'correlation')[0, 0]\n",
    "        ]\n",
    "        feature_vector.extend(glcm_feats)\n",
    "\n",
    "        # --- 3. HOG Features (192 features) ---\n",
    "        # \n",
    "        # orientations=8: 8 directions for gradients\n",
    "        # pixels_per_cell=(16, 16): Each cell is 16x16 pixels\n",
    "        # cells_per_block=(1, 1): Each block is 1x1 cells\n",
    "        # (100/16 = 6.25 -> 6 cells wide, 75/16 = 4.68 -> 4 cells high)\n",
    "        # Total blocks = 6 * 4 = 24\n",
    "        # Total features = 24 blocks * 1x1 cells/block * 8 orientations = 192\n",
    "        hog_features = hog(gray_cell, \n",
    "                           orientations=8, \n",
    "                           pixels_per_cell=(16, 16),\n",
    "                           cells_per_block=(1, 1), \n",
    "                           visualize=False, \n",
    "                           block_norm='L2-Hys')\n",
    "        feature_vector.extend(hog_features)\n",
    "\n",
    "        # --- 4. Edge Density (1 feature) ---\n",
    "        # Use standard Canny thresholds\n",
    "        edges = cv2.Canny(cell, threshold1=100, threshold2=200)\n",
    "        # Calculate mean (percentage of edge pixels)\n",
    "        edge_density = np.mean(edges) / 255.0\n",
    "        feature_vector.append(edge_density)\n",
    "\n",
    "        # --- 5. Harris Corner Count (1 feature) ---\n",
    "        # blockSize=2, ksize=3, k=0.04 are standard values\n",
    "        dst = cv2.cornerHarris(gray_cell, blockSize=2, ksize=3, k=0.04)\n",
    "        # Count corners by thresholding the response\n",
    "        harris_corner_count = np.sum(dst > 0.01 * dst.max())\n",
    "        feature_vector.append(harris_corner_count)\n",
    "\n",
    "        # --- 6. FAST Keypoint Count (1 feature) ---\n",
    "        fast = cv2.FastFeatureDetector_create()\n",
    "        keypoints = fast.detect(gray_cell, None)\n",
    "        fast_keypoint_count = len(keypoints)\n",
    "        feature_vector.append(fast_keypoint_count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features from a cell: {e}\")\n",
    "        # Return None or a vector of zeros if extraction fails\n",
    "        return None\n",
    "\n",
    "    # Return the final, flat vector of 205 features\n",
    "    return np.array(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_images_2d.shape: (431, 64, 75, 100, 3)\n",
      "y_labels_2d.shape: (431, 64)\n"
     ]
    }
   ],
   "source": [
    "# # filenames now like: \"<image_id>_<cellno>\" e.g. \"1244_55.jpg\" or \"1244_55\"\n",
    "# pat = re.compile(r\"(?P<imgid>.+?)_(?P<cell>\\d+)(?:\\..+)?$\")\n",
    "\n",
    "# groups = defaultdict(list)\n",
    "# for img_arr, fname in zip(cell_loaded_images, cell_image_filenames):\n",
    "#     m = pat.match(fname)\n",
    "#     if not m:\n",
    "#         # try basename fallback\n",
    "#         m = pat.match(os.path.basename(fname))\n",
    "#     if not m:\n",
    "#         # skip files that don't follow the new naming convention\n",
    "#         continue\n",
    "#     imgid = m.group(\"imgid\")\n",
    "#     cell_no = int(m.group(\"cell\"))  # numeric cell index\n",
    "#     # label lookup using filename (try exact, then basename)\n",
    "#     label = label_map.get(fname, label_map.get(os.path.basename(fname), np.nan))\n",
    "#     groups[imgid].append((cell_no, img_arr, label, fname))\n",
    "\n",
    "# # Build rows: only keep original images that have exactly 64 cells, sorted by cell number\n",
    "# cell_images_rows = []\n",
    "# label_rows = []\n",
    "# for imgid, items in groups.items():\n",
    "#     if len(items) != 64:\n",
    "#         # skip incomplete images (or change this logic to pad/handle differently)\n",
    "#         continue\n",
    "#     # sort by numeric cell index (assumes cell indices 0..63 or 1..64)\n",
    "#     items_sorted = sorted(items, key=lambda x: x[0])\n",
    "#     imgs = [np.asarray(it[1], dtype=np.uint8) for it in items_sorted]\n",
    "#     labs = [int(it[2]) if not pd.isna(it[2]) else 0 for it in items_sorted]\n",
    "#     cell_images_rows.append(imgs)\n",
    "#     label_rows.append(labs)\n",
    "\n",
    "# # Final 2D arrays: each row corresponds to one original image (64 cells)\n",
    "# cell_images_2d = np.array(cell_images_rows, dtype=object)  # shape (num_images, 64)\n",
    "# y_labels_2d = np.array(label_rows, dtype=int)              # shape (num_images, 64)\n",
    "\n",
    "# print(\"cell_images_2d.shape:\", cell_images_2d.shape)\n",
    "# print(\"y_labels_2d.shape:\", y_labels_2d.shape)\n",
    "# # ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rebuild_and_mark(cells, labels, grid_size=8, green_value=200, tint_alpha=0.6):\n",
    "#     \"\"\"\n",
    "#     Apply a full green overlay to cells with label==1.\n",
    "#     tint_alpha: 0.0 = no tint, 1.0 = full green\n",
    "#     green_value: 0-255 intensity for green channel (BGR)\n",
    "#     \"\"\"\n",
    "#     cells_list = [np.asarray(c, dtype=np.uint8) for c in list(cells)]\n",
    "#     num_cells = len(cells_list)\n",
    "#     assert num_cells == grid_size * grid_size, \"Expected 64 cells\"\n",
    "\n",
    "#     h, w = cells_list[0].shape[:2]\n",
    "#     channels = 3\n",
    "#     H, W = h * grid_size, w * grid_size\n",
    "#     final_image = np.zeros((H, W, channels), dtype=np.uint8)\n",
    "\n",
    "#     # green tint image (BGR)\n",
    "#     tint_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "#     tint_img[:] = (0, green_value, 0)\n",
    "\n",
    "#     idx = 0\n",
    "#     for row in range(grid_size):\n",
    "#         for col in range(grid_size):\n",
    "#             cell = cells_list[idx].copy()\n",
    "#             # ensure 3-channel uint8\n",
    "#             if cell.ndim == 2:\n",
    "#                 cell = cv2.cvtColor(cell, cv2.COLOR_GRAY2BGR)\n",
    "#             elif cell.shape[2] == 1:\n",
    "#                 cell = cv2.cvtColor(cell, cv2.COLOR_GRAY2BGR)\n",
    "#             cell = cell.astype(np.uint8)\n",
    "\n",
    "#             if labels[idx] == 1:\n",
    "#                 # full-cell green overlay\n",
    "#                 cell = cv2.addWeighted(cell, 1.0 - tint_alpha, tint_img, tint_alpha, 0)\n",
    "\n",
    "#             final_image[row*h:(row+1)*h, col*w:(col+1)*w] = cell\n",
    "#             idx += 1\n",
    "\n",
    "#     return final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93df350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ...existing code...\n",
    "# # fix LABEL_OUTPUT_FOLDER stray quote\n",
    "# LABEL_OUTPUT_FOLDER = 'Labeled_images'\n",
    "# label_dir = os.path.join(LABEL_OUTPUT_FOLDER)\n",
    "# os.makedirs(label_dir, exist_ok=True)\n",
    "# for cells, labels in zip(cell_images_2d, y_labels_2d):\n",
    "#     image = rebuild_and_mark(cells, labels)\n",
    "#     cv2.imwrite(os.path.join(label_dir, f\"labeled_image_{np.random.randint(1e6)}.png\"), image)\n",
    "# # ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e99b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction for all cells...\n",
      "---\n",
      "Feature extraction complete!\n",
      "Feature matrix 'X_data' shape: (27584, 205)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting feature extraction for all cells...\")\n",
    "\n",
    "# X_data will be your feature matrix (e.g., 12800 rows x 205 columns)\n",
    "X_data = []\n",
    "for image, filename in zip(cell_loaded_images,cell_image_filenames):\n",
    "        \n",
    "        # Extract the 205 features from the 100x75 cell\n",
    "        features = extract_all_features(image)\n",
    "        \n",
    "        if features is not None:\n",
    "            # Add the feature vector to our dataset\n",
    "            X_data.append(features)\n",
    "\n",
    "# Convert X_data to a NumPy array for efficiency\n",
    "X_data = np.array(X_data)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Feature extraction complete!\")\n",
    "print(f\"Feature matrix 'X_data' shape: {X_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da24348",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training Model ---\")\n",
    "\n",
    "# --- 1. Split Data into Training and Testing Sets ---\n",
    "# We'll use 80% for training, 20% for testing\n",
    "y_data = y_labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_data  # Ensures both sets get a similar % of 0s and 1s\n",
    ")\n",
    "\n",
    "print(f\"Original training data: {X_train.shape[0]} samples\")\n",
    "print(f\"Test data: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(\"Handling data imbalance with SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"New resampled training data: {X_train_resampled.shape[0]} samples\")\n",
    "print(f\"Wildlife (1s) in new training set: {np.sum(y_train_resampled)}\")\n",
    "print(f\"Background (0s) in new training set: {len(y_train_resampled) - np.sum(y_train_resampled)}\")\n",
    "\n",
    "# --- 3. Train the Random Forest Classifier ---\n",
    "rf_model = RandomForestClassifier(n_estimators=100, \n",
    "                                random_state=42, \n",
    "                                n_jobs=-1)\n",
    "\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Draft model training complete.\")\n",
    "\n",
    "print(\"\\n--- Draft Model Evaluation (on Test Set) ---\")\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['Background (0)', 'Wildlife (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n--- Training SVM Model ---\")\n",
    "\n",
    "y_data = y_labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_data\n",
    ")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"SVM model training complete.\")\n",
    "\n",
    "print(\"\\n--- SVM Model Evaluation (on Test Set) ---\")\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svm, target_names=['Background (0)', 'Wildlife (1)']))\n",
    "\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "print(cm_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db13d08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyotirya-agrawal/Desktop/DS203/Assignment 8/venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:17:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model training complete.\n",
      "\n",
      "--- XGBoost Model Evaluation (on Test Set) ---\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Background (0)       0.90      0.91      0.90      4287\n",
      "  Wildlife (1)       0.66      0.64      0.65      1230\n",
      "\n",
      "      accuracy                           0.85      5517\n",
      "     macro avg       0.78      0.77      0.78      5517\n",
      "  weighted avg       0.84      0.85      0.85      5517\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[3880  407]\n",
      " [ 441  789]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_data = y_labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_data\n",
    ")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=300, \n",
    "                            random_state=42, \n",
    "                            n_jobs=-1, \n",
    "                            use_label_encoder=False, \n",
    "                            eval_metric='logloss')\n",
    "\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"XGBoost model training complete.\")\n",
    "\n",
    "print(\"\\n--- XGBoost Model Evaluation (on Test Set) ---\")\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['Background (0)', 'Wildlife (1)']))\n",
    "\n",
    "print(\"XGBoost Confusion Matrix:\")\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(cm_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Global Env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
